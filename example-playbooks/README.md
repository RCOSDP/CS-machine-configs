## Example Playbooks

This directory provides self-contained Ansible playbooks to provision
a cluster on mdx.

- jupyterlab-cluster.yaml: Spawn jupyter lab on all VMs, and configure
  a VM, which would have a global IPv4 address by DNAT, as a reverse
  proxy to access the jupyter lab instances.

- mpi-cluster.yaml: Provision a cluster capable of MPI (OpenMPI
  installed along with OFED).


Getting Started will be documented on docs.mdx.jp...


To execude ansible-playbook, we need an inventory
file. `mdxcsv2inventory.py` helps you to generate an inventory from a
CSV file generated by the mdx user portal.

```shell-session
$ ./mdxcsv2inventory.py -h
usage: mdxcsv2inventory.py [-h] [-6] [-u ANSIBLE_USER] [-g DEFAULT_GROUP]
                           [--per-node-groups]
                           [--group-with GROUP [VM_NAME ...]]
                           [--group-without GROUP [VM_NAME ...]]
                           [--output OUTPUT]
                           csv

positional arguments:
  csv                   CSV file generated by mdx user portal

optional arguments:
  -h, --help            show this help message and exit
  -6, --use-ipv6        use IPv6 address for hosts
  -u ANSIBLE_USER, --ansible-user ANSIBLE_USER
                        user to run ansible, default is mdxuser
  -g DEFAULT_GROUP, --default-group DEFAULT_GROUP
                        group name for all nodes, default is default
  --per-node-groups     make per-node groups in the inventory
  --group-with GROUP [VM_NAME ...]
                        make a group with specified VM names
  --group-without GROUP [VM_NAME ...]
                        make a group without specified VM names
  --output OUTPUT       output file name, default is STDOUT
```

By default, it generates a simple inventory (to STDOUT).

The following output shows an inventory from a CSV for VMs deployed
with `vm[1-8]`.

```shell-session
$ ./mdxcsv2inventory.py user-portal-vm-info.csv
[all:vars]
ansible_user=mdxuser
ethipv4prefix=10.13.200.0/21
rdmaipv4prefix=10.141.200.0/21

[default]
10.13.200.118   hostname=vm1 ethipv4=10.13.200.118   rdmaipv4=10.141.204.34  
10.13.204.51    hostname=vm2 ethipv4=10.13.204.51    rdmaipv4=10.141.200.119 
10.13.204.47    hostname=vm3 ethipv4=10.13.204.47    rdmaipv4=10.141.204.35  
10.13.204.53    hostname=vm4 ethipv4=10.13.204.53    rdmaipv4=10.141.204.41  
10.13.200.121   hostname=vm5 ethipv4=10.13.200.121   rdmaipv4=10.141.200.122 
10.13.200.122   hostname=vm6 ethipv4=10.13.200.122   rdmaipv4=10.141.200.120 
10.13.204.48    hostname=vm7 ethipv4=10.13.204.48    rdmaipv4=10.141.204.38  
10.13.204.52    hostname=vm8 ethipv4=10.13.204.52    rdmaipv4=10.141.204.40 

```

You can generate host groups with/without specified VMs. For example,
to configure `vm1` as a manager and others as workers for an MPI
cluster:

```shell-session
$ ./mdxcsv2inventory.py user-portal-vm-info.csv --group-with manager vm1 --group-without workers vm1
[all:vars]
ansible_user=mdxuser
ethipv4prefix=10.13.200.0/21
rdmaipv4prefix=10.141.200.0/21

[default]
10.13.200.118   hostname=vm1 ethipv4=10.13.200.118   rdmaipv4=10.141.204.34  
10.13.204.51    hostname=vm2 ethipv4=10.13.204.51    rdmaipv4=10.141.200.119 
10.13.204.47    hostname=vm3 ethipv4=10.13.204.47    rdmaipv4=10.141.204.35  
10.13.204.53    hostname=vm4 ethipv4=10.13.204.53    rdmaipv4=10.141.204.41  
10.13.200.121   hostname=vm5 ethipv4=10.13.200.121   rdmaipv4=10.141.200.122 
10.13.200.122   hostname=vm6 ethipv4=10.13.200.122   rdmaipv4=10.141.200.120 
10.13.204.48    hostname=vm7 ethipv4=10.13.204.48    rdmaipv4=10.141.204.38  
10.13.204.52    hostname=vm8 ethipv4=10.13.204.52    rdmaipv4=10.141.204.40  

[manager]
# group with vm1
10.13.200.118   hostname=vm1 ethipv4=10.13.200.118   rdmaipv4=10.141.204.34  

[workers]
# group without vm1
10.13.204.51    hostname=vm2 ethipv4=10.13.204.51    rdmaipv4=10.141.200.119 
10.13.204.47    hostname=vm3 ethipv4=10.13.204.47    rdmaipv4=10.141.204.35  
10.13.204.53    hostname=vm4 ethipv4=10.13.204.53    rdmaipv4=10.141.204.41  
10.13.200.121   hostname=vm5 ethipv4=10.13.200.121   rdmaipv4=10.141.200.122 
10.13.200.122   hostname=vm6 ethipv4=10.13.200.122   rdmaipv4=10.141.200.120 
10.13.204.48    hostname=vm7 ethipv4=10.13.204.48    rdmaipv4=10.141.204.38  
10.13.204.52    hostname=vm8 ethipv4=10.13.204.52    rdmaipv4=10.141.204.40

```

The above group names fit mpi-cluster.yaml. The following commands
provison an MPI cluster based on the CSV file.

```shell-session
./mdxcsv2inventory.py user-portal-vm-info.csv --group-with manager vm1 \
  					      --group-without workers vm1 \
					      > mpi-hosts.ini

ansible-playbook -i mpi-hosts.ini mpi-cluster.yaml
```

After provisioning finished, ssh (probably with ssh-agent) to vm1
(manager) as mdxuser, execute `cat /etc/hosts | grep rdma | awk
'{print $2}' | ssh-keyscan -f - >> ~/.ssh/known_hosts` to gather ssh
server fingerpints, and then you can do `mpirun -np 4 -H
vm2-rdma,vm3-rdma,vm4-rdma,vm5-rdma
/nfs/osu-micro-benchmarks-5.8/mpi/collective/osu_gather` for example.


For jupyterlab-cluster.yaml:

```shell-session
./mdxcsv2inventory.py user-portal-vm-info.csv --group-with nginx vm1 \
		      			      --group-with nfsserver vm1 \
					      --group-without nfsclient vm1 \
					      > jupyter-hosts.ini

ansible-playbook -i jupyter-hosts.ini jupyterlab-cluster.yaml
```

Assign a global IPv4 address to `vm1` by DNAT, and then you can access
jupyterlab on a VM through `http://[vm1 global addr]:8001` for
example.

`ansible-playbook -f FORKS` would contribute speed up for provisioning.